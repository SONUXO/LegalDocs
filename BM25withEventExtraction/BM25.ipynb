{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, re, string, codecs, random, numpy as np, pandas as pd\n",
    "#import evaluate_at_K\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import pickle as pkl\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25(object):\n",
    "    def __init__(self, b=0.7, k1=1.6, n_gram:int = 1):\n",
    "        self.n_gram = n_gram\n",
    "        self.vectorizer = TfidfVectorizer(max_df=.65, min_df=1,\n",
    "                                  use_idf=True, \n",
    "                                  ngram_range=(n_gram, n_gram))\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        print(f\"Fitting tf_idf vectorizer\")\n",
    "\n",
    "        y = self.vectorizer.fit_transform(X)  # Combine fit and transform\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "        print(f\"Finished tf_idf vectorizer, time : {time.perf_counter() - start_time:0.3f} sec\")\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        if not q.strip():  # Handle empty queries\n",
    "            return np.zeros(len(X))  \n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        \n",
    "        if q.nnz == 0:  # If query has no tokens in vocabulary\n",
    "            return np.zeros(len(X))  \n",
    "\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "candidate_query = pd.read_csv('data/train_candidates.csv')  # Added `.csv`\n",
    "corpus = list(candidate_query[\"text\"])  # List of candidate case texts\n",
    "citation_names = list(candidate_query[\"id\"].astype(str))  # List of case IDs (string format)\n",
    "\n",
    "train_query = pd.read_csv('data/train_queries.csv')  # Added `.csv`\n",
    "query_corpus = list(train_query[\"text\"])  # List of query case texts\n",
    "query_names = list(train_query[\"id\"].astype(str))  # List of query case IDs (string format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert `relevant_candidates` into a dictionary\n",
    "true_labels = {}\n",
    "for index, row in train_query.iterrows():\n",
    "    query_id = str(row[\"id\"])  \n",
    "    relevant_cases = row[\"relevant_candidates\"]\n",
    "\n",
    "    if isinstance(relevant_cases, str):  \n",
    "        relevant_cases = ast.literal_eval(relevant_cases)  \n",
    "    \n",
    "    if isinstance(relevant_cases, float):  \n",
    "        relevant_cases = []\n",
    "    \n",
    "    true_labels[query_id] = list(map(str, relevant_cases))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tf_idf vectorizer\n",
      "Finished tf_idf vectorizer, time : 11.206 sec\n"
     ]
    }
   ],
   "source": [
    "# Train BM25 Model\n",
    "bm25 = BM25(n_gram=1)\n",
    "bm25.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BM25 scores\n",
    "bm_25_results_dict = {}\n",
    "for i in range(len (query_corpus)):\n",
    "    qu = query_corpus[i]\n",
    "    qu_n = query_names[i]\n",
    "    doc_scores = bm25.transform(qu, corpus)\n",
    "    bm_25_results_dict[qu_n] = {citation_names[j]: doc_scores[j] for j in range(len(doc_scores))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "bm_25_results_list = []\n",
    "for query_id, scores in bm_25_results_dict.items():\n",
    "    for doc_id, score in scores.items():\n",
    "        bm_25_results_list.append([query_id, doc_id, score])\n",
    "bm_25_results_df = pd.DataFrame(bm_25_results_list, columns=['query_case_id', 'candidate_case_id', 'bm25_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_results_df = pd.read_csv(\"results/bm25_similarity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ground truth labels to true_labels.json\n"
     ]
    }
   ],
   "source": [
    "true_labels_json_path = \"true_labels.json\"\n",
    "\n",
    "true_labels_dict = {\n",
    "    \"Query Set\": [{\"id\": qid, \"relevant candidates\": rel_cases} for qid, rel_cases in true_labels.items()],\n",
    "    \"Candidate Set\": [{\"id\": str(doc_id)} for doc_id in bm_25_results_df[\"candidate_case_id\"].unique()]\n",
    "}\n",
    "\n",
    "with open(true_labels_json_path, \"w\") as f:\n",
    "    json.dump(true_labels_dict, f, indent=4)\n",
    "\n",
    "print(f\"saved ground truth labels to {true_labels_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING FOR FAILURE OF k_similarties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"results/bm25_similarity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all present\n"
     ]
    }
   ],
   "source": [
    "train_unique_id = set(train_query[\"id\"].unique())\n",
    "result_unique_id = set(results[\"query_case_id\"].unique())\n",
    "\n",
    "missing_values = train_unique_id - result_unique_id\n",
    "\n",
    "if missing_values:\n",
    "    print(\"missings: \")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"all present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not present\n"
     ]
    }
   ],
   "source": [
    "val = 1980953300\n",
    "\n",
    "if val in (train_unique_id and result_unique_id) :\n",
    "    print(\"present\")\n",
    "else:\n",
    "    print(\"not present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of train queries 827\n",
      "the length of train result 827\n"
     ]
    }
   ],
   "source": [
    "print(f\"the length of train queries {len(train_unique_id)}\")\n",
    "print(f\"the length of train result {len(result_unique_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
