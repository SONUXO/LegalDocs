{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Precision, Recall, and F1 scores at K\n",
    "def get_micro_scores_at_K(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "\n",
    "    correctly_retrieved = len(act_set & pred_set)  \n",
    "    relevant_cases = len(act_set)  \n",
    "    retrieved_cases = k  \n",
    "\n",
    "    return correctly_retrieved, relevant_cases, retrieved_cases\n",
    "\n",
    "# Function to calculate F1, Precision, and Recall vs K\n",
    "def get_f1_vs_K(gold_labels, similarity_df):\n",
    "    precision_vs_K = []\n",
    "    recall_vs_K = []\n",
    "    f1_vs_K = []\n",
    "\n",
    "    for k in tqdm(range(1, 21)):  # Iterate over K from 1 to 20\n",
    "        correct_retrieved_all = []\n",
    "        relevant_cases_all = []\n",
    "        retrieved_cases_all = []\n",
    "\n",
    "        for query_case_id in similarity_df.query_case_id.values:\n",
    "            if query_case_id not in [1864396, 1508893]:  # Exclude specific cases\n",
    "                gold = gold_labels[gold_labels[\"query_case_id\"].values == query_case_id].values[0][1:]\n",
    "                actual = np.asarray(list(gold_labels.columns)[1:])[np.logical_or(gold == 1, gold == -2)]\n",
    "                actual = [str(i) for i in actual]\n",
    "\n",
    "                candidate_docs = [int(i) for i in gold_labels.columns.values[1:]]\n",
    "                column_name = 'query_case_id' if 'query_case_id' in similarity_df.columns else 'Unnamed: 0'\n",
    "                \n",
    "                similarity_scores = similarity_df[similarity_df[column_name].values == query_case_id].values[0][1:]\n",
    "                query_case_id = int(query_case_id)  # Ensure it's a Python int\n",
    "\n",
    "                sorted_candidates = [x for _, x in sorted(zip(similarity_scores, candidate_docs), key=lambda pair: float(pair[0]), reverse=True)]\n",
    "\n",
    "                if query_case_id not in candidate_docs:\n",
    "                    print(f\"⚠️ query_case_id {query_case_id} is missing from candidate_docs\")\n",
    "                elif query_case_id in sorted_candidates:\n",
    "                    sorted_candidates.remove(query_case_id)\n",
    "                else:\n",
    "                    print(f\"⚠️ query_case_id {query_case_id} is missing from sorted_candidates after sorting\")\n",
    "\n",
    "                sorted_candidates = [str(i) for i in sorted_candidates]\n",
    "\n",
    "                # Compute scores\n",
    "                correctly_retrieved, relevant_cases, retrieved_cases = get_micro_scores_at_K(actual, sorted_candidates, k)\n",
    "                correct_retrieved_all.append(correctly_retrieved)\n",
    "                relevant_cases_all.append(relevant_cases)\n",
    "                retrieved_cases_all.append(retrieved_cases)\n",
    "\n",
    "        # Compute final Precision, Recall, and F1-score at K\n",
    "        recall_score = np.sum(correct_retrieved_all) / np.sum(relevant_cases_all)\n",
    "        precision_score = np.sum(correct_retrieved_all) / np.sum(retrieved_cases_all)\n",
    "        \n",
    "        f1_score = 0 if recall_score == 0 or precision_score == 0 else (2 * precision_score * recall_score) / (precision_score + recall_score)\n",
    "\n",
    "        recall_vs_K.append(recall_score)\n",
    "        precision_vs_K.append(precision_score)\n",
    "        f1_vs_K.append(f1_score)\n",
    "\n",
    "    return {\"recall_vs_K\": recall_vs_K, \"precision_vs_K\": precision_vs_K, \"f1_vs_K\": f1_vs_K}\n",
    "\n",
    "# Function to process labels from JSON file\n",
    "def obtain_sim_df_from_labels(labels):\n",
    "    query_numbers = [int(re.findall(r'\\d+', i[\"id\"])[0]) for i in labels[\"Query Set\"]]\n",
    "    \n",
    "    # Ensure we filter only valid numerical candidates\n",
    "    relevant_cases = []\n",
    "    for i in labels[\"Query Set\"]:\n",
    "        cleaned_cases = [re.findall(r'\\d+', j) for j in i[\"relevant candidates\"]]\n",
    "        cleaned_cases = [int(j[0]) for j in cleaned_cases if j]  # Only take non-empty matches\n",
    "        relevant_cases.append(cleaned_cases)\n",
    "\n",
    "    relevant_cases = {i: j for i, j in zip(query_numbers, relevant_cases)}\n",
    "\n",
    "    candidate_numbers = [int(re.findall(r'\\d+', i[\"id\"])[0]) for i in labels[\"Candidate Set\"]]\n",
    "    candidate_numbers.sort()\n",
    "\n",
    "    row_wise_dataframe = {}\n",
    "    for query_number in sorted(list(relevant_cases.keys())):\n",
    "        relevance_dict = {}\n",
    "        for candidate in candidate_numbers:\n",
    "            relevance_dict[candidate] = -1 if candidate == query_number else (1 if candidate in relevant_cases[query_number] else 0)\n",
    "        row_wise_dataframe[query_number] = relevance_dict\n",
    "\n",
    "    df = pd.DataFrame(row_wise_dataframe).T\n",
    "    df.insert(loc=0, column='query_case_id', value=row_wise_dataframe.keys())\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load True Labels JSON\n",
    "with open(\"true_labels.json\", 'r') as f:\n",
    "    true_labels = json.load(f)\n",
    "\n",
    "gold_labels_df = obtain_sim_df_from_labels(true_labels)  # Convert JSON to DataFrame\n",
    "\n",
    "# Load BM25 Similarity CSV\n",
    "sim_df = pd.read_csv(\"bm25_similarity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics\n",
    "results = get_f1_vs_K(gold_labels_df, sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision, Recall, and F1-score vs K\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 21), results[\"precision_vs_K\"], label=\"Precision\", marker=\"o\", linestyle=\"dashed\")\n",
    "plt.plot(range(1, 21), results[\"recall_vs_K\"], label=\"Recall\", marker=\"s\", linestyle=\"dashed\")\n",
    "plt.plot(range(1, 21), results[\"f1_vs_K\"], label=\"F1-score\", marker=\"^\", linestyle=\"dashed\")\n",
    "\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, and F1-score vs K\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
